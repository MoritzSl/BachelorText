\chapter{Experimente und Ergebnisse}
In diesem Kapitel werden verschiedene Experimente mit dem FPGA durchgeführt. Zunächst wird gemessen, Wie viel Zeit pro trainiertem Perzeptron für das Training und die Voraussagen auf dem FPGA und einer Implementierung auf dem Hostsystem aufgewendet werden muss. Hierbei wird das Programm auf dem Hostsystem nicht parallelisiert. 
Dann werden beispielhaft Trainingsvorgänge mit verschiedenen Datensätzen durchgeführt, um die Genauigkeit der Vorhersagen zu überprüfen und Ergebnisse der Testvorgänge zu präsentieren.\\
Es werden für die Experimente die Datensätze MNIST \cite{MNIST}, und verwendet.\\
Die Features $\vec f$ des jeweiligen Datensatzes wurden normalisiert, so dass $\forall f \in \vec f\text{ . } f \in [0,1]$. 
\section{Regularisierung}
Mit dem MNIST Datensatz wurden verschiedene Testzyklen zur Regularisierung durchgeführt. Ein Trainingsdurchlauf mit 200 Variablen und 100 Wiederholungen führte zu folgenden Ergebnissen: Hierbei wurde der Datensatz so angepasst, dass alle Variablen die die Ziffer "`3"' darstellen mit "`trifft zu"' und alle anderen Variablen mit "`trifft nicht zu"' markiert wurden.\\\\
\begin{tabularx}{\textwidth}{p{0.16\textwidth}|r|r|r|r|r}
Perceptron & Regularisierungsrate & Lernrate & Batch & Methode & Genauigkeit\\
\hline
a & 0.0 & 0.100098 & 1 & keine & 95.43\%\\
\hline
b & 0.007812 & 0.100098 & 1 & L2 & 95.75\%\\
\hline
c & 0.009766 & 0.100098 & 1 & L2 & 95.75\%\\
\hline
d & 0.5 & 0.100098 & 1 & L2 & 89.9\%\\
\hline
e & 0 & 0.100098 & 1 & L1 & 95.43\%\\
\hline
f & 0.000977 & 0.100098 & 1 & L1 & 95.43\%\\
\hline
g & 0.004883 & 0.100098 & 1 & L1 & 94.38\%\\
\hline
h & 0.009766 & 0.100098 & 1 & L1 & 93.38\%\\
\end{tabularx}

