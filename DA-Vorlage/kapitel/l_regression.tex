\chapter{Logistische Regression}
Dieses Kapitel befasst sich mit der Logistischen Regression. Zunächst wird die Definition und Funktion der Logistischen Regression erklärt. Danach wird die Geschichte und Entwicklung der Methode erörtert. Des weiteren gibt es eine Vertiefung der verschiedenen Regularisierungsmethoden und zum Abschluss die Grenzen der Funktion sowie einen Ausblick auf verwandte Algorithmen.
\section{Definition und Funktion}
Die logistische Regression ist ein statistisches Analyseverfahren, bei dem es darum geht, eine Beziehung zwischen einer abhängigen und mehrerer unabhängiger Variablen zu modellieren und wird auch als als binäres Logit-Modell bezeichnet.
Sie unterscheidet sich in soweit von der linearen Regression, dass die Voraussagen nicht spezielle Werte, sondern die Wahrscheinlichkeiten angeben, mit denen die jeweilige Ausprägung der Variable angenommen wird.\cite{ROHR} Die beiden Ausprägungen der abhängigen Variablen wird mit 0 bzw. 1 beschrieben, sodass die Vorhersage des Modells die Wahrscheinlichkeit beschreibt, mit der die abhängige Variable den Wert 1 annimmt, formal $P(Y_i=1)$ . Die Logistische Regression gehört zur Klasse der strukturen-prüfenden Verfahren und bildet eine Variation der Regressionsanalyse. Sie grenzt sich durch die Art ihrer abhängigen Variable, bezeichnet mit Y, welche als kategoriale Variable klassifiziert ist, von anderen Regressionsanalysen ab. Die Ausprägungen der Variable repräsentieren die verschiedenen Alternativen, in unserem binären (oder auch dichotomen) Fall ist "`trifft zu"' und "`trifft nicht zu"'.\cite{BECK}
Diese Gruppen werden nun mit 0 und 1 bezeichnet und für die Y Variable gilt nun: 
\begin{displaymath}
P(Y=0)=1-P(Y=1)\text{ und } P(Y=1)=1-P(Y=0)
\end{displaymath}
Ziel der Logistischen Regression ist es, gegeben Trainingsdaten 
$D = \{(X_1, y_1), \dots, (X_N,y_N)\}$ mit $X_i \in \mathbb R^d$ und $y_i \in 
\{0,1\}$ , ein Modell $f_{\beta}(x)$ für Vorhersagen finden, 
welches auf neuen, ungesehenen Daten einen möglichst kleinen Fehler macht.
Ausdrücken lässt sich das logistische Regressionsmodell nun wie folgt:
\begin{displaymath}
\pi(x) = f_{\beta}(x_{1},..,x_{n})
\end{displaymath}
Wobei $\pi(X_i)=P(Y=1|X_i)$ die bedingte Wahrscheinlichkeit, unter der das Ereignis 1 ("`trift zu"') mit den gegebenen Werten $X_i = (x_{i1},...,x_{id})^{T}$ eintritt, angibt.\\
Wie auch bei der Linearen Regression werden hierbei die unabhängigen Variablen linear miteinander kombiniert. Die sogenannte systematische Komponente des Modells wird durch die Linearkombination
\begin{displaymath}
z(X_i)=\beta_0 + \sum_{j=1}^{d}{\beta_j * x_{ij}} + u_i
\end{displaymath}
beschrieben. $\beta$ stellt hier den Vektor der Koeffizienten $(\beta_1 ,..., \beta_d)^T$ dar und $\beta_0$ ist der Bias. $u_i$ ist ein zu vernachlässigender Störterm.\cite{ROHR}
Um das Modell auszugestalten wird hier die logistische Funktion 
\begin{displaymath}
p=\dfrac{\exp(x)}{1+\exp(x)}=\dfrac{1}{1+\exp(-x)}
\end{displaymath}
verwendet.\cite{BECK}
In Abbildung 3.1 sieht man den s-förmigen Verlauf der Funktion. Dieser Verlauf, als Verteilungsfunktion interpretiert, approximiert die Verteilungsfunktion der Normalverteilung mit ausreichender Genauigkeit. Somit kann sie verwendet werden um reellwertige Variablen (im Wertebereich $[-\infty, +\infty]$) auf eine Wahrscheinlichkeit (im Wertebereich $[0,1]$) zu transformieren, denn die Verteilungsfunktion der Normalverteilung ist nur als Integral auszudrücken und damit schwer zu berechnen.\cite{WIKI}\cite{BECK}
\begin{figure}[ht]
\centering
\includegraphics[scale=0.65]{bilder/logistic_reg_func}
\caption{Die logistische Funktion $p=\dfrac{1}{1+\exp(-x)}$ }
\end{figure}
\newpage
Wenn man diese Transformation der systematischen Komponente nun mit der logistischen Funktion durchführt erhält man die logistische Regressionsfunktion:
\begin{displaymath}
\pi(X)= \dfrac{1}{1+\exp(z(X))}
\end{displaymath}
Also genauer:
\begin{displaymath}
P(Y=1 | X=x_i)=P(Y_i=1)=\frac{\exp(\beta_0+x_i^T\beta)}{1+\exp(\beta_0+x_i^T\beta)} = \frac{1}{1+\exp(-(\beta_0+x_i^T\beta))}
\end{displaymath}
Die systemische Komponente $z(X)=\beta_0 + x_i^T\beta$ ist ein Prädiktor für $\pi(X)$. Je größer $z(X)$, desto größer auch $\pi(X)$ und damit auch $P(Y=1|X)$.\cite{BECK} 
\section{Lernen mit Logistischer Regression}
Da das logistische Regressionsmodell dazu verwendet werden soll anhand gelernter Trainingsdaten Voraussagen für weitere, unbekannte Daten zu berechnen, ist es notwendig alle unbekannten Variablen entsprechend dem vorliegenden Datensatz anzupassen.
Um mit dem Modell möglichst korrekte Voraussagen treffen zu können müssen hierfür der Vektor der Koeffizienten $\beta$ und der Bias $\beta_0$ geschickt gewählt werden. Die Koeffizienten geben dabei die Bedeutsamkeit der einzelnen Ausprägungen der Variablen $X$ an, je größer $|\beta_i|, i \in 1..d$ desto größer sind auch die Auswirkungen der jeweiligen Ausprägung auf die Entscheidung.\cite{FS}
In der linearen Regression wird hierfür häufig die "`Methode der kleinsten Quadrate"' 
\begin{displaymath}
\sum_{i=1}^N(\pi(X_i)-y_i)^2
\end{displaymath}
gewählt. Es werden die Werte für $\beta$ gewählt, welche möglichst kleine quadrierte Fehler machen. Somit ergibt sich die Minimierungsfunktion: 
\begin{displaymath}
\min_\beta \sum_{i_1}^N (\pi(X_i)-y_i)^2
\end{displaymath}
Unter den üblichen Annahmen der linearen Regression ist die "`Summe der kleinsten Quadrate"' eine gute Schätzfunktion mit brauchbaren statistischen Eigenschaften.\cite{WIL}
Unter den Annahmen der logistischen Regression verliert diese Methode jedoch diese Eigenschaften. Die am häufigsten gewählte Methode die "`Summe der kleinsten Quadrate"' zu berechnen ist, unter Annahme dass die Fehlerterme normalverteilt sind, die Maximum Likelihood. Diese passt die unbekannten Variablen so an, dass die Chance, mit ihnen die gegebenen Daten darzustellen maximiert wird. Dies bildet auch die Grundlage zur Findung der unbekannten Variablen der logistischen Regression. Um diese Variablen bestimmen zu können bedarf es einer Funktion, der sogenannten Maximum Likeliehood Funktion. Sie drückt die Wahrscheinlichkeit aus in wie weit die gegebenen Daten die Variablen als Funktion darstellen. Die Maximum Likelihood Schätzer der Parameter sind dabei die Werte welche diese Funktion maximieren.\cite{WIL}

Die Anpassung der Koeffizienten erfolgt durch Minimierung der Loss Funktion: \begin{displaymath}
\min_\beta \sum_{i}^N \log(1+\exp(-(\beta_0+x_i^T\beta))) +C*R(\beta)
\end{displaymath}
Wobei $C \in \mathbb R$ ein Hyperparameter ist,der fest gewählt werden muss. Dies geschieht zumeist durch das Testen verschiedener Werte, sodass hier ein Ansatz zur Parallelisierung entsteht.
\section{Stochastic Gradient Descent}
\section{Regularisierungsmethoden}
\subsection{LASSO}
L1-Regularisierung (Least Absolute Shrinkage and Selection Operator, kurz \textbf{LASSO}) mit $R(\beta) = \sum_{i}^N|\beta_i|$
\subsection{Ridge Regression}
L2-Regularisierung (Ridge Regression) mit $R(\beta)= \sum_{i}^N \beta_i^2$.
\section{Verwandte Algorithmen}